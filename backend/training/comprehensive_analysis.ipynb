{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPORTS ====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SETUP DIRECTORIES ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SETUP - DETECTING ENVIRONMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detect environment\n",
    "try:\n",
    "    output_dir = \"/kaggle/working/comprehensive_analysis\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    is_kaggle = True\n",
    "    print(\"‚úì Running on Kaggle\")\n",
    "except:\n",
    "    output_dir = os.path.join(r\"c:\\Users\\almir\\ai-privacy\\backend\", \"comprehensive_analysis\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    is_kaggle = False\n",
    "    print(\"‚úì Running locally\")\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0614bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LOAD ALL RESULTS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING ALL EXPERIMENTAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {'baseline': None, 'fl': None, 'dp': None}\n",
    "\n",
    "# Helper function to find files\n",
    "def find_file(filename, kaggle_pattern=None, local_paths=None):\n",
    "    \"\"\"Try to find file in Kaggle input or local paths\"\"\"\n",
    "    if is_kaggle and kaggle_pattern:\n",
    "        paths = glob.glob(kaggle_pattern)\n",
    "        if paths:\n",
    "            return paths[0]\n",
    "    \n",
    "    if local_paths:\n",
    "        for path in local_paths:\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Load Baseline Results\n",
    "baseline_path = find_file(\n",
    "    'research_results.json',\n",
    "    kaggle_pattern='/kaggle/input/*/research_results.json',\n",
    "    local_paths=[\n",
    "        r'c:\\Users\\almir\\ai-privacy\\backend\\models_research\\research_results.json',\n",
    "        r'c:\\Users\\almir\\ai-privacy\\backend\\models_research_fl_dp\\research_results.json'\n",
    "    ]\n",
    ")\n",
    "\n",
    "if baseline_path:\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        results['baseline'] = json.load(f)\n",
    "    print(f\"‚úì Baseline results loaded from: {baseline_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Baseline results not found\")\n",
    "\n",
    "# Load FL Results\n",
    "fl_path = find_file(\n",
    "    'fl_adult_results.json',\n",
    "    kaggle_pattern='/kaggle/input/*/fl_adult_results.json',\n",
    "    local_paths=[\n",
    "        r'c:\\Users\\almir\\ai-privacy\\backend\\models_fl_adult\\fl_adult_results.json',\n",
    "        r'c:\\Users\\almir\\ai-privacy\\backend\\models_fl_continue\\fl_adult_results.json'\n",
    "    ]\n",
    ")\n",
    "\n",
    "if fl_path:\n",
    "    with open(fl_path, 'r') as f:\n",
    "        results['fl'] = json.load(f)\n",
    "    print(f\"‚úì FL results loaded from: {fl_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  FL results not found\")\n",
    "\n",
    "# Load DP Results\n",
    "dp_path = find_file(\n",
    "    'dp_continue_results.json',\n",
    "    kaggle_pattern='/kaggle/input/*/dp_continue_results.json',\n",
    "    local_paths=[\n",
    "        r'c:\\Users\\almir\\ai-privacy\\backend\\models_research_dp_continue\\dp_continue_results.json',\n",
    "        r'c:\\Users\\almir\\ai-privacy\\backend\\models_research_dp\\dp_results.json'\n",
    "    ]\n",
    ")\n",
    "\n",
    "if dp_path:\n",
    "    with open(dp_path, 'r') as f:\n",
    "        results['dp'] = json.load(f)\n",
    "    print(f\"‚úì DP results loaded from: {dp_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  DP results not found\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Available results:\")\n",
    "for key, value in results.items():\n",
    "    status = \"‚úì\" if value else \"‚úó\"\n",
    "    print(f\"  {status} {key.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457491c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EXTRACT BASELINE DATA ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING BASELINE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_summary = []\n",
    "\n",
    "if results['baseline']:\n",
    "    for dataset in ['diabetes', 'adult']:\n",
    "        for model in ['LR', 'FNN']:\n",
    "            data = results['baseline']['baseline_results'][dataset][model]\n",
    "            baseline_summary.append({\n",
    "                'Dataset': dataset,\n",
    "                'Model': model,\n",
    "                'Method': 'Baseline',\n",
    "                'Accuracy': data['accuracy']['mean'] * 100,\n",
    "                'Std': data['accuracy']['std'] * 100,\n",
    "                'Min': data['accuracy']['min'] * 100,\n",
    "                'Max': data['accuracy']['max'] * 100,\n",
    "                'F1': data['f1']['mean'] * 100,\n",
    "                'F1_Std': data['f1']['std'] * 100\n",
    "            })\n",
    "    \n",
    "    baseline_df = pd.DataFrame(baseline_summary)\n",
    "    print(\"\\n\" + baseline_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping - baseline results not available\")\n",
    "    baseline_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EXTRACT FL DATA ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING FEDERATED LEARNING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fl_summary = []\n",
    "\n",
    "if results['fl']:\n",
    "    for config_key, config_data in results['fl']['federated_learning'].items():\n",
    "        fl_summary.append({\n",
    "            'Dataset': config_data['dataset'],\n",
    "            'Model': config_data['model'],\n",
    "            'Method': f\"FL-{config_data['aggregation']}\",\n",
    "            'Aggregation': config_data['aggregation'],\n",
    "            'Accuracy': config_data['accuracy']['mean'] * 100,\n",
    "            'Std': config_data['accuracy']['std'] * 100,\n",
    "            'Min': config_data['accuracy']['min'] * 100,\n",
    "            'Max': config_data['accuracy']['max'] * 100,\n",
    "            'F1': config_data['f1']['mean'] * 100,\n",
    "            'F1_Std': config_data['f1']['std'] * 100\n",
    "        })\n",
    "    \n",
    "    fl_df = pd.DataFrame(fl_summary)\n",
    "    print(\"\\n\" + fl_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping - FL results not available\")\n",
    "    fl_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EXTRACT DP DATA ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING DIFFERENTIAL PRIVACY DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dp_summary = []\n",
    "\n",
    "if results['dp']:\n",
    "    for config_key, config_data in results['dp']['differential_privacy'].items():\n",
    "        dp_summary.append({\n",
    "            'Dataset': config_data['dataset'],\n",
    "            'Model': config_data['model'],\n",
    "            'Method': f\"DP-Œµ{config_data['target_epsilon']}\",\n",
    "            'Epsilon': config_data['target_epsilon'],\n",
    "            'Actual_Epsilon': config_data['actual_epsilon'],\n",
    "            'Accuracy': config_data['accuracy']['mean'] * 100,\n",
    "            'Std': config_data['accuracy']['std'] * 100,\n",
    "            'Min': config_data['accuracy']['min'] * 100,\n",
    "            'Max': config_data['accuracy']['max'] * 100,\n",
    "            'F1': config_data['f1']['mean'] * 100,\n",
    "            'F1_Std': config_data['f1']['std'] * 100\n",
    "        })\n",
    "    \n",
    "    dp_df = pd.DataFrame(dp_summary)\n",
    "    print(\"\\n\" + dp_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping - DP results not available\")\n",
    "    dp_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STATISTICAL COMPARISONS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL COMPARISONS - T-TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparisons = []\n",
    "\n",
    "if results['baseline']:\n",
    "    # FL vs Baseline\n",
    "    if results['fl']:\n",
    "        print(\"\\nüìä Federated Learning vs Baseline:\")\n",
    "        for config_key, fl_config in results['fl']['federated_learning'].items():\n",
    "            dataset = fl_config['dataset']\n",
    "            model = fl_config['model']\n",
    "            baseline_key = f\"{dataset}_{model}\"\n",
    "            \n",
    "            if baseline_key in results['baseline']['baseline_results'][dataset]:\n",
    "                baseline_data = results['baseline']['baseline_results'][dataset][model]\n",
    "                baseline_acc = baseline_data['accuracy']['mean'] * 100\n",
    "                baseline_all = baseline_data['all_accuracies']\n",
    "                \n",
    "                fl_acc = fl_config['accuracy']['mean'] * 100\n",
    "                fl_all = fl_config['all_accuracies']\n",
    "                \n",
    "                t_stat, p_value = stats.ttest_ind(baseline_all, fl_all)\n",
    "                \n",
    "                comparisons.append({\n",
    "                    'Comparison': f\"FL-{fl_config['aggregation']} vs Baseline\",\n",
    "                    'Dataset': dataset,\n",
    "                    'Model': model,\n",
    "                    'Method_Acc': fl_acc,\n",
    "                    'Baseline_Acc': baseline_acc,\n",
    "                    'Difference': fl_acc - baseline_acc,\n",
    "                    't_stat': t_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "                })\n",
    "    \n",
    "    # DP vs Baseline\n",
    "    if results['dp']:\n",
    "        print(\"\\nüìä Differential Privacy vs Baseline:\")\n",
    "        for config_key, dp_config in results['dp']['differential_privacy'].items():\n",
    "            dataset = dp_config['dataset']\n",
    "            model = dp_config['model']\n",
    "            \n",
    "            if dataset in results['baseline']['baseline_results']:\n",
    "                baseline_data = results['baseline']['baseline_results'][dataset][model]\n",
    "                baseline_acc = baseline_data['accuracy']['mean'] * 100\n",
    "                baseline_all = baseline_data['all_accuracies']\n",
    "                \n",
    "                dp_acc = dp_config['accuracy']['mean'] * 100\n",
    "                dp_all = dp_config['all_accuracies']\n",
    "                \n",
    "                t_stat, p_value = stats.ttest_ind(baseline_all, dp_all)\n",
    "                \n",
    "                comparisons.append({\n",
    "                    'Comparison': f\"DP-Œµ{dp_config['target_epsilon']} vs Baseline\",\n",
    "                    'Dataset': dataset,\n",
    "                    'Model': model,\n",
    "                    'Method_Acc': dp_acc,\n",
    "                    'Baseline_Acc': baseline_acc,\n",
    "                    'Difference': dp_acc - baseline_acc,\n",
    "                    't_stat': t_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "                })\n",
    "\n",
    "if comparisons:\n",
    "    comparison_df = pd.DataFrame(comparisons)\n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Save comparisons\n",
    "    comparison_df.to_csv(os.path.join(output_dir, 'statistical_comparisons.csv'), index=False)\n",
    "    print(f\"\\n‚úì Saved: statistical_comparisons.csv\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No comparisons possible - baseline results not available\")\n",
    "    comparison_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b11538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION 1: OVERALL COMPARISON ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 1: Overall Performance Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not baseline_df.empty or not fl_df.empty or not dp_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Comprehensive Privacy-Preserving ML Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    datasets = ['diabetes', 'adult']\n",
    "    models = ['LR', 'FNN']\n",
    "    \n",
    "    for idx, (dataset, model) in enumerate([(d, m) for d in datasets for m in models]):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        plot_data = []\n",
    "        labels = []\n",
    "        colors = []\n",
    "        \n",
    "        # Baseline\n",
    "        if not baseline_df.empty:\n",
    "            base = baseline_df[(baseline_df['Dataset'] == dataset) & (baseline_df['Model'] == model)]\n",
    "            if not base.empty:\n",
    "                plot_data.append((base['Accuracy'].values[0], base['Std'].values[0]))\n",
    "                labels.append('Baseline')\n",
    "                colors.append('red')\n",
    "        \n",
    "        # FL methods\n",
    "        if not fl_df.empty:\n",
    "            fl_subset = fl_df[(fl_df['Dataset'] == dataset) & (fl_df['Model'] == model)]\n",
    "            for _, row in fl_subset.iterrows():\n",
    "                plot_data.append((row['Accuracy'], row['Std']))\n",
    "                labels.append(row['Aggregation'])\n",
    "                colors.append('blue')\n",
    "        \n",
    "        # DP methods (just show a few key epsilon values)\n",
    "        if not dp_df.empty:\n",
    "            dp_subset = dp_df[(dp_df['Dataset'] == dataset) & (dp_df['Model'] == model)]\n",
    "            # Select representative epsilon values\n",
    "            for eps in [0.5, 3.0, 10.0]:\n",
    "                dp_eps = dp_subset[dp_subset['Epsilon'] == eps]\n",
    "                if not dp_eps.empty:\n",
    "                    plot_data.append((dp_eps['Accuracy'].values[0], dp_eps['Std'].values[0]))\n",
    "                    labels.append(f'DP-Œµ{eps}')\n",
    "                    colors.append('green')\n",
    "        \n",
    "        if plot_data:\n",
    "            x_pos = np.arange(len(plot_data))\n",
    "            accs = [d[0] for d in plot_data]\n",
    "            stds = [d[1] for d in plot_data]\n",
    "            \n",
    "            ax.bar(x_pos, accs, yerr=stds, capsize=5, alpha=0.7, color=colors)\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "            ax.set_ylabel('Accuracy (%)')\n",
    "            ax.set_title(f'{dataset.upper()} - {model}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz1_path = os.path.join(output_dir, '1_overall_comparison.png')\n",
    "    plt.savefig(viz1_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: 1_overall_comparison.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ed4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION 2: FL AGGREGATION METHODS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 2: FL Aggregation Methods Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not fl_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('Federated Learning: Aggregation Methods Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, dataset in enumerate(['diabetes', 'adult']):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for model in ['LR', 'FNN']:\n",
    "            subset = fl_df[(fl_df['Dataset'] == dataset) & (fl_df['Model'] == model)]\n",
    "            \n",
    "            if not subset.empty:\n",
    "                x = range(len(subset))\n",
    "                ax.errorbar(x, subset['Accuracy'], yerr=subset['Std'], \n",
    "                           marker='o', capsize=5, label=model, linewidth=2, markersize=8)\n",
    "        \n",
    "        # Add baseline if available\n",
    "        if not baseline_df.empty:\n",
    "            for model, color in [('LR', 'blue'), ('FNN', 'orange')]:\n",
    "                base = baseline_df[(baseline_df['Dataset'] == dataset) & (baseline_df['Model'] == model)]\n",
    "                if not base.empty:\n",
    "                    ax.axhline(y=base['Accuracy'].values[0], color=color, \n",
    "                              linestyle='--', alpha=0.5, label=f'{model} Baseline')\n",
    "        \n",
    "        ax.set_xticks(range(5))\n",
    "        ax.set_xticklabels(['FedAvg', 'FedProx', 'q-FedAvg', 'SCAFFOLD', 'FedAdam'], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'{dataset.upper()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz2_path = os.path.join(output_dir, '2_fl_aggregation_comparison.png')\n",
    "    plt.savefig(viz2_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: 2_fl_aggregation_comparison.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  FL results not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89896a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION 3: DP PRIVACY-ACCURACY TRADEOFF ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 3: DP Privacy-Accuracy Tradeoff\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not dp_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.suptitle('Differential Privacy: Privacy-Accuracy Tradeoff', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, dataset in enumerate(['diabetes', 'adult']):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for model in ['LR', 'FNN']:\n",
    "            subset = dp_df[(dp_df['Dataset'] == dataset) & (dp_df['Model'] == model)]\n",
    "            \n",
    "            if not subset.empty:\n",
    "                subset_sorted = subset.sort_values('Epsilon')\n",
    "                ax.errorbar(subset_sorted['Epsilon'], subset_sorted['Accuracy'], \n",
    "                           yerr=subset_sorted['Std'], marker='o', capsize=5, \n",
    "                           label=model, linewidth=2, markersize=8)\n",
    "        \n",
    "        # Add baseline if available\n",
    "        if not baseline_df.empty:\n",
    "            for model, color in [('LR', 'blue'), ('FNN', 'orange')]:\n",
    "                base = baseline_df[(baseline_df['Dataset'] == dataset) & (baseline_df['Model'] == model)]\n",
    "                if not base.empty:\n",
    "                    ax.axhline(y=base['Accuracy'].values[0], color=color, \n",
    "                              linestyle='--', alpha=0.5, label=f'{model} Baseline')\n",
    "        \n",
    "        ax.set_xlabel('Privacy Budget (Œµ)')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'{dataset.upper()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz3_path = os.path.join(output_dir, '3_dp_privacy_accuracy_tradeoff.png')\n",
    "    plt.savefig(viz3_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: 3_dp_privacy_accuracy_tradeoff.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  DP results not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION 4: ACCURACY LOSS HEATMAP ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 4: Accuracy Loss from Baseline (Heatmap)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    # Pivot for heatmap\n",
    "    heatmap_data = comparison_df.pivot_table(\n",
    "        index='Comparison', \n",
    "        columns=['Dataset', 'Model'], \n",
    "        values='Difference'\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdYlGn', center=0, \n",
    "                ax=ax, cbar_kws={'label': 'Accuracy Difference (%)'})\n",
    "    ax.set_title('Accuracy Loss/Gain from Baseline (Negative = Loss)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Method')\n",
    "    ax.set_xlabel('Dataset - Model')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz4_path = os.path.join(output_dir, '4_accuracy_loss_heatmap.png')\n",
    "    plt.savefig(viz4_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: 4_accuracy_loss_heatmap.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Comparison data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE SUMMARY TABLES ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING SUMMARY TABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all results\n",
    "all_results = []\n",
    "if not baseline_df.empty:\n",
    "    all_results.append(baseline_df)\n",
    "if not fl_df.empty:\n",
    "    all_results.append(fl_df)\n",
    "if not dp_df.empty:\n",
    "    all_results.append(dp_df)\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    combined_df.to_csv(os.path.join(output_dir, 'all_results_summary.csv'), index=False)\n",
    "    print(f\"‚úì Saved: all_results_summary.csv ({len(combined_df)} rows)\")\n",
    "\n",
    "# Save individual summaries\n",
    "if not baseline_df.empty:\n",
    "    baseline_df.to_csv(os.path.join(output_dir, 'baseline_summary.csv'), index=False)\n",
    "    print(f\"‚úì Saved: baseline_summary.csv\")\n",
    "\n",
    "if not fl_df.empty:\n",
    "    fl_df.to_csv(os.path.join(output_dir, 'fl_summary.csv'), index=False)\n",
    "    print(f\"‚úì Saved: fl_summary.csv\")\n",
    "\n",
    "if not dp_df.empty:\n",
    "    dp_df.to_csv(os.path.join(output_dir, 'dp_summary.csv'), index=False)\n",
    "    print(f\"‚úì Saved: dp_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ COMPREHENSIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll outputs saved to: {output_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in os.listdir(output_dir):\n",
    "    print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
