{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd14f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n",
      "\n",
      "Configuration:\n",
      "  Random seeds: [42, 123, 456, 789, 1011]\n",
      "  K-fold splits: 5\n",
      "  Total evaluations per config: 5 × 5 = 25\n"
     ]
    }
   ],
   "source": [
    "# ==================== IMPORTS ====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from copy import deepcopy\n",
    "import kagglehub\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEEDS = [42, 123, 456, 789, 1011]\n",
    "N_SPLITS = 5\n",
    "NUM_CLIENTS = 5\n",
    "LOCAL_EPOCHS = 5\n",
    "GLOBAL_ROUNDS = 20\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Random seeds: {RANDOM_SEEDS}\")\n",
    "print(f\"  K-fold splits: {N_SPLITS}\")\n",
    "print(f\"  Total evaluations per config: {len(RANDOM_SEEDS)} × {N_SPLITS} = {len(RANDOM_SEEDS) * N_SPLITS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b77f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DIABETES DATASET FROM KAGGLE\n",
      "================================================================================\n",
      "✓ Using kagglehub for dataset download\n",
      "✓ Diabetes dataset loaded: (253680, 22)\n"
     ]
    }
   ],
   "source": [
    "# ==================== LOAD DIABETES DATASET ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DIABETES DATASET FROM KAGGLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    diabetes_paths = glob.glob('/kaggle/input/*/diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "    \n",
    "    if diabetes_paths:\n",
    "        diabetes_csv = diabetes_paths[0]\n",
    "        print(\"✓ Using Kaggle native dataset path\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Dataset not found in /kaggle/input/\")\n",
    "        \n",
    "except (FileNotFoundError, IndexError):\n",
    "    print(\"✓ Using kagglehub for dataset download\")\n",
    "    diabetes_path = kagglehub.dataset_download(\"alexteboul/diabetes-health-indicators-dataset\")\n",
    "    diabetes_csv = f\"{diabetes_path}/diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
    "\n",
    "df_diabetes = pd.read_csv(diabetes_csv)\n",
    "print(f\"✓ Diabetes dataset loaded: {df_diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0654fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING DATA\n",
      "================================================================================\n",
      "✓ Diabetes - Features: (253680, 21), Target: (253680,)\n"
     ]
    }
   ],
   "source": [
    "# ==================== PREPROCESS DATA ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_diabetes = df_diabetes.drop(columns=['Diabetes_binary']).values\n",
    "y_diabetes = df_diabetes['Diabetes_binary'].values\n",
    "print(f\"✓ Diabetes - Features: {X_diabetes.shape}, Target: {y_diabetes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a9c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODEL ARCHITECTURES ====================\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64], output_size=2, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"✓ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a12bb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FL utilities defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== FEDERATED LEARNING UTILITIES ====================\n",
    "\n",
    "def fedavg_aggregate(client_models, client_data_sizes):\n",
    "    \"\"\"FedAvg: Weighted average based on client data sizes\"\"\"\n",
    "    total_size = sum(client_data_sizes)\n",
    "    global_state = {}\n",
    "    \n",
    "    for key in client_models[0].state_dict().keys():\n",
    "        global_state[key] = sum(\n",
    "            client_models[i].state_dict()[key] * (client_data_sizes[i] / total_size)\n",
    "            for i in range(len(client_models))\n",
    "        )\n",
    "    \n",
    "    return global_state\n",
    "\n",
    "def fedprox_aggregate(client_models, client_data_sizes):\n",
    "    \"\"\"FedProx: Similar to FedAvg\"\"\"\n",
    "    return fedavg_aggregate(client_models, client_data_sizes)\n",
    "\n",
    "def qfedavg_aggregate(client_models, client_losses, q=0.2):\n",
    "    \"\"\"q-FedAvg: Fairness-weighted aggregation\"\"\"\n",
    "    lipschitz = [1.0 / (loss + 1e-10) for loss in client_losses]\n",
    "    weights = [l ** q for l in lipschitz]\n",
    "    total_weight = sum(weights)\n",
    "    \n",
    "    global_state = {}\n",
    "    for key in client_models[0].state_dict().keys():\n",
    "        global_state[key] = sum(\n",
    "            client_models[i].state_dict()[key] * (weights[i] / total_weight)\n",
    "            for i in range(len(client_models))\n",
    "        )\n",
    "    \n",
    "    return global_state\n",
    "\n",
    "def scaffold_aggregate(client_models, client_data_sizes):\n",
    "    \"\"\"SCAFFOLD: Simplified version\"\"\"\n",
    "    return fedavg_aggregate(client_models, client_data_sizes)\n",
    "\n",
    "# FedAdam state\n",
    "fedadam_state = {'m': None, 'v': None, 't': 0}\n",
    "\n",
    "def fedadam_aggregate(client_models, client_data_sizes, beta1=0.9, beta2=0.999, eta=0.01, tau=1e-3):\n",
    "    \"\"\"FedAdam: Adaptive federated optimization\"\"\"\n",
    "    total_size = sum(client_data_sizes)\n",
    "    avg_state = {}\n",
    "    \n",
    "    device = next(client_models[0].parameters()).device\n",
    "    \n",
    "    for key in client_models[0].state_dict().keys():\n",
    "        avg_state[key] = sum(\n",
    "            client_models[i].state_dict()[key] * (client_data_sizes[i] / total_size)\n",
    "            for i in range(len(client_models))\n",
    "        )\n",
    "    \n",
    "    if fedadam_state['m'] is None:\n",
    "        fedadam_state['m'] = {key: torch.zeros_like(val).to(device) for key, val in avg_state.items()}\n",
    "        fedadam_state['v'] = {key: torch.zeros_like(val).to(device) for key, val in avg_state.items()}\n",
    "    \n",
    "    fedadam_state['t'] += 1\n",
    "    global_state = {}\n",
    "    \n",
    "    for key in avg_state.keys():\n",
    "        delta = avg_state[key]\n",
    "        fedadam_state['m'][key] = fedadam_state['m'][key].to(device)\n",
    "        fedadam_state['v'][key] = fedadam_state['v'][key].to(device)\n",
    "        \n",
    "        fedadam_state['m'][key] = beta1 * fedadam_state['m'][key] + (1 - beta1) * delta\n",
    "        fedadam_state['v'][key] = beta2 * fedadam_state['v'][key] + (1 - beta2) * (delta ** 2)\n",
    "        \n",
    "        m_hat = fedadam_state['m'][key] / (1 - beta1 ** fedadam_state['t'])\n",
    "        v_hat = fedadam_state['v'][key] / (1 - beta2 ** fedadam_state['t'])\n",
    "        \n",
    "        global_state[key] = avg_state[key] + eta * m_hat / (torch.sqrt(v_hat) + tau)\n",
    "    \n",
    "    return global_state\n",
    "\n",
    "def train_fl_client(client_model, train_loader, global_model=None, epochs=5, lr=0.001, mu=0.01, use_proximal=False):\n",
    "    \"\"\"Train a single FL client\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    client_model = client_model.to(device)\n",
    "    \n",
    "    if use_proximal and global_model is not None:\n",
    "        global_model = global_model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(client_model.parameters(), lr=lr)\n",
    "    \n",
    "    client_model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = client_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            if use_proximal and global_model is not None:\n",
    "                proximal_term = 0.0\n",
    "                for w, w_t in zip(client_model.parameters(), global_model.parameters()):\n",
    "                    proximal_term += (w - w_t).norm(2)\n",
    "                loss += (mu / 2) * proximal_term\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        total_loss += epoch_loss / len(train_loader)\n",
    "    \n",
    "    return client_model, total_loss / epochs\n",
    "\n",
    "def distribute_data_to_clients(X, y, num_clients, batch_size):\n",
    "    \"\"\"Distribute data IID to clients\"\"\"\n",
    "    dataset = TensorDataset(torch.FloatTensor(X), torch.LongTensor(y))\n",
    "    \n",
    "    total_size = len(dataset)\n",
    "    client_sizes = [total_size // num_clients] * num_clients\n",
    "    client_sizes[-1] += total_size % num_clients\n",
    "    \n",
    "    indices = torch.randperm(total_size).tolist()\n",
    "    client_loaders = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for size in client_sizes:\n",
    "        client_indices = indices[start_idx:start_idx + size]\n",
    "        client_dataset = Subset(dataset, client_indices)\n",
    "        client_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        client_loaders.append(client_loader)\n",
    "        start_idx += size\n",
    "    \n",
    "    return client_loaders, client_sizes\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate model and return accuracy, f1\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    accuracy = accuracy_score(y, predicted.cpu().numpy())\n",
    "    f1 = f1_score(y, predicted.cpu().numpy(), average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "print(\"✓ FL utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEDERATED LEARNING - DIABETES FNN CONTINUATION\n",
      "================================================================================\n",
      "\n",
      "Results will be saved to: /kaggle/working/models_fl_diabetes_continue\n",
      "\n",
      "Configuration: Diabetes FNN - 3 aggregation methods\n",
      "  Remaining: ['q-FedAvg', 'SCAFFOLD', 'FedAdam']\n",
      "\n",
      "  Aggregation: q-FedAvg\n",
      "    Run 1, Fold 5: Acc=0.8656\n",
      "    Run 2, Fold 5: Acc=0.8656\n",
      "    Run 3, Fold 5: Acc=0.8650\n",
      "    Run 4, Fold 5: Acc=0.8644\n",
      "    Run 5, Fold 5: Acc=0.8652\n",
      "\n",
      "  ✓ q-FedAvg Results:\n",
      "    Accuracy: 86.46% ± 0.08% (range: 86.32% - 86.61%)\n",
      "    F1-Score: 83.71% ± 0.17%\n",
      "\n",
      "  Aggregation: SCAFFOLD\n",
      "    Run 1, Fold 5: Acc=0.8650\n",
      "    Run 2, Fold 5: Acc=0.8652\n",
      "    Run 3, Fold 5: Acc=0.8643\n",
      "    Run 4, Fold 5: Acc=0.8643\n",
      "    Run 5, Fold 5: Acc=0.8657\n",
      "\n",
      "  ✓ SCAFFOLD Results:\n",
      "    Accuracy: 86.44% ± 0.10% (range: 86.22% - 86.62%)\n",
      "    F1-Score: 83.74% ± 0.15%\n",
      "\n",
      "  Aggregation: FedAdam\n",
      "    Run 1, Fold 5: Acc=0.8647\n",
      "    Run 2, Fold 5: Acc=0.8647\n",
      "    Run 3, Fold 5: Acc=0.8644\n"
     ]
    }
   ],
   "source": [
    "# ==================== FL CONTINUATION - DIABETES FNN ONLY ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEDERATED LEARNING - DIABETES FNN CONTINUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    models_dir = \"/kaggle/working/models_fl_diabetes_continue\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "except:\n",
    "    models_dir = os.path.join(r\"c:\\Users\\almir\\ai-privacy\\backend\", \"models_fl_diabetes_continue\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nResults will be saved to: {models_dir}\")\n",
    "\n",
    "# Only run FNN with remaining aggregations\n",
    "MODEL_TYPE = 'FNN'\n",
    "REMAINING_AGGREGATIONS = ['q-FedAvg', 'SCAFFOLD', 'FedAdam']\n",
    "\n",
    "print(f\"\\nConfiguration: Diabetes FNN - 3 aggregation methods\")\n",
    "print(f\"  Remaining: {REMAINING_AGGREGATIONS}\")\n",
    "\n",
    "fl_results = {}\n",
    "\n",
    "for agg_method in REMAINING_AGGREGATIONS:\n",
    "    print(f\"\\n  Aggregation: {agg_method}\")\n",
    "    \n",
    "    all_accuracies = []\n",
    "    all_f1s = []\n",
    "    \n",
    "    # Reset FedAdam state\n",
    "    fedadam_state['m'] = None\n",
    "    fedadam_state['v'] = None\n",
    "    fedadam_state['t'] = 0\n",
    "    \n",
    "    for run_idx, seed in enumerate(RANDOM_SEEDS):\n",
    "        skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_diabetes, y_diabetes)):\n",
    "            X_train, X_val = X_diabetes[train_idx], X_diabetes[val_idx]\n",
    "            y_train, y_val = y_diabetes[train_idx], y_diabetes[val_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            \n",
    "            # Distribute data to clients\n",
    "            client_loaders, client_sizes = distribute_data_to_clients(\n",
    "                X_train_scaled, y_train, NUM_CLIENTS, BATCH_SIZE\n",
    "            )\n",
    "            \n",
    "            # Initialize global model (FNN)\n",
    "            input_size = X_train_scaled.shape[1]\n",
    "            global_model = FeedforwardNN(input_size, hidden_sizes=[128, 64], output_size=2)\n",
    "            \n",
    "            # FL Training\n",
    "            for round_num in range(GLOBAL_ROUNDS):\n",
    "                client_models = []\n",
    "                client_losses = []\n",
    "                \n",
    "                for client_id in range(NUM_CLIENTS):\n",
    "                    client_model = deepcopy(global_model)\n",
    "                    use_proximal = (agg_method == 'FedProx')\n",
    "                    \n",
    "                    trained_model, loss = train_fl_client(\n",
    "                        client_model, client_loaders[client_id],\n",
    "                        global_model=global_model if use_proximal else None,\n",
    "                        epochs=LOCAL_EPOCHS, lr=LEARNING_RATE, use_proximal=use_proximal\n",
    "                    )\n",
    "                    \n",
    "                    client_models.append(trained_model)\n",
    "                    client_losses.append(loss)\n",
    "                \n",
    "                # Aggregate\n",
    "                if agg_method == 'FedAvg':\n",
    "                    global_state = fedavg_aggregate(client_models, client_sizes)\n",
    "                elif agg_method == 'FedProx':\n",
    "                    global_state = fedprox_aggregate(client_models, client_sizes)\n",
    "                elif agg_method == 'q-FedAvg':\n",
    "                    global_state = qfedavg_aggregate(client_models, client_losses)\n",
    "                elif agg_method == 'SCAFFOLD':\n",
    "                    global_state = scaffold_aggregate(client_models, client_sizes)\n",
    "                elif agg_method == 'FedAdam':\n",
    "                    global_state = fedadam_aggregate(client_models, client_sizes)\n",
    "                \n",
    "                global_model.load_state_dict(global_state)\n",
    "            \n",
    "            # Evaluate\n",
    "            accuracy, f1 = evaluate_model(global_model, X_val_scaled, y_val)\n",
    "            all_accuracies.append(accuracy)\n",
    "            all_f1s.append(f1)\n",
    "            \n",
    "            if fold_idx == N_SPLITS - 1:\n",
    "                print(f\"    Run {run_idx + 1}, Fold {fold_idx + 1}: Acc={accuracy:.4f}\")\n",
    "    \n",
    "    # Statistics\n",
    "    acc_mean = np.mean(all_accuracies)\n",
    "    acc_std = np.std(all_accuracies, ddof=1)\n",
    "    acc_min = np.min(all_accuracies)\n",
    "    acc_max = np.max(all_accuracies)\n",
    "    f1_mean = np.mean(all_f1s)\n",
    "    f1_std = np.std(all_f1s, ddof=1)\n",
    "    \n",
    "    config_key = f\"diabetes_{MODEL_TYPE}_{agg_method}\"\n",
    "    fl_results[config_key] = {\n",
    "        'dataset': 'diabetes',\n",
    "        'model': MODEL_TYPE,\n",
    "        'aggregation': agg_method,\n",
    "        'accuracy': {'mean': acc_mean, 'std': acc_std, 'min': acc_min, 'max': acc_max},\n",
    "        'f1': {'mean': f1_mean, 'std': f1_std},\n",
    "        'all_accuracies': all_accuracies,\n",
    "        'all_f1s': all_f1s\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  ✓ {agg_method} Results:\")\n",
    "    print(f\"    Accuracy: {acc_mean*100:.2f}% ± {acc_std*100:.2f}% (range: {acc_min*100:.2f}% - {acc_max*100:.2f}%)\")\n",
    "    print(f\"    F1-Score: {f1_mean*100:.2f}% ± {f1_std*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FL CONTINUATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b12644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_json = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dataset': 'diabetes',\n",
    "        'model': 'FNN',\n",
    "        'random_seeds': RANDOM_SEEDS,\n",
    "        'n_splits': N_SPLITS,\n",
    "        'total_evaluations': len(RANDOM_SEEDS) * N_SPLITS,\n",
    "        'note': 'Continuation - q-FedAvg, SCAFFOLD, FedAdam only'\n",
    "    },\n",
    "    'federated_learning': fl_results\n",
    "}\n",
    "\n",
    "json_path = os.path.join(models_dir, 'fl_diabetes_fnn_continue.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results_json, f, indent=2, default=lambda x: float(x) if isinstance(x, np.floating) else x)\n",
    "print(f\"✓ Saved: fl_diabetes_fnn_continue.json\")\n",
    "\n",
    "# Summary CSV\n",
    "summary_data = []\n",
    "for config_key, config_data in fl_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': config_data['model'],\n",
    "        'Aggregation': config_data['aggregation'],\n",
    "        'Accuracy': config_data['accuracy']['mean'] * 100,\n",
    "        'Std': config_data['accuracy']['std'] * 100,\n",
    "        'Min': config_data['accuracy']['min'] * 100,\n",
    "        'Max': config_data['accuracy']['max'] * 100,\n",
    "        'F1': config_data['f1']['mean'] * 100\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "csv_path = os.path.join(models_dir, 'fl_diabetes_fnn_summary.csv')\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Saved: fl_diabetes_fnn_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL RESULTS SAVED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985746ba",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "\n",
    "1. **Diabetes FNN - 3 aggregation methods**: q-FedAvg, SCAFFOLD, FedAdam\n",
    "2. **5-fold CV × 5 runs** = 25 evaluations per method (75 total)\n",
    "3. **Results saved** to `/kaggle/working/models_fl_diabetes_continue/`\n",
    "\n",
    "**Next steps:**\n",
    "- Download `fl_diabetes_fnn_continue.json`\n",
    "- Merge with main FL results\n",
    "- Run comprehensive analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
