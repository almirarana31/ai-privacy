{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n",
      "\n",
      "Configuration:\n",
      "  Random seeds: [42, 123, 456, 789, 1011]\n",
      "  K-fold splits: 5\n",
      "  Privacy budgets (Îµ): [0.5, 1.0, 3.0, 5.0, 10.0]\n",
      "  Total evaluations per config: 5 Ã— 5 = 25\n"
     ]
    }
   ],
   "source": [
    "# ==================== IMPORTS ====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import kagglehub\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from opacus import PrivacyEngine\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# Suppress Opacus warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='opacus')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEEDS = [42, 123, 456, 789, 1011]\n",
    "N_SPLITS = 5\n",
    "DP_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_VALUES = [0.5, 1.0, 3.0, 5.0, 10.0]\n",
    "DP_NOISE_MULTIPLIER = 1.0\n",
    "DP_MAX_GRAD_NORM = 1.0\n",
    "DP_DELTA = 1e-5\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Random seeds: {RANDOM_SEEDS}\")\n",
    "print(f\"  Total evaluations per config: {len(RANDOM_SEEDS)} Ã— {N_SPLITS} = {len(RANDOM_SEEDS) * N_SPLITS}\")\n",
    "print(f\"  K-fold splits: {N_SPLITS}\")\n",
    "print(f\"  Privacy budgets (Îµ): {EPSILON_VALUES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85338f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATASETS FROM KAGGLE\n",
      "================================================================================\n",
      "âœ“ Using kagglehub for dataset download\n",
      "âœ“ Diabetes dataset loaded: (253680, 22)\n",
      "âœ“ Adult dataset loaded: (32561, 15)\n"
     ]
    }
   ],
   "source": [
    "# ==================== LOAD DATASETS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATASETS FROM KAGGLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try Kaggle's native dataset access first (for Kaggle notebooks)\n",
    "try:\n",
    "    # On Kaggle, datasets are mounted at /kaggle/input/\n",
    "    diabetes_paths = glob.glob('/kaggle/input/*/diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "    adult_paths = glob.glob('/kaggle/input/*/adult.csv')\n",
    "    \n",
    "    if diabetes_paths and adult_paths:\n",
    "        diabetes_csv = diabetes_paths[0]\n",
    "        adult_csv = adult_paths[0]\n",
    "        print(\"âœ“ Using Kaggle native dataset paths\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Datasets not found in /kaggle/input/\")\n",
    "        \n",
    "except (FileNotFoundError, IndexError):\n",
    "    # Fallback to kagglehub for local execution\n",
    "    print(\"âœ“ Using kagglehub for dataset download\")\n",
    "    \n",
    "    diabetes_path = kagglehub.dataset_download(\"alexteboul/diabetes-health-indicators-dataset\")\n",
    "    diabetes_csv = f\"{diabetes_path}/diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
    "    \n",
    "    adult_path = kagglehub.dataset_download(\"uciml/adult-census-income\")\n",
    "    adult_csv = f\"{adult_path}/adult.csv\"\n",
    "\n",
    "# Load datasets\n",
    "df_diabetes = pd.read_csv(diabetes_csv)\n",
    "print(f\"âœ“ Diabetes dataset loaded: {df_diabetes.shape}\")\n",
    "\n",
    "df_adult = pd.read_csv(adult_csv)\n",
    "print(f\"âœ“ Adult dataset loaded: {df_adult.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c34a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING DATA\n",
      "================================================================================\n",
      "âœ“ Diabetes - Features: (253680, 21), Target: (253680,)\n",
      "âœ“ Adult - Features: (32561, 14), Target: (32561,)\n"
     ]
    }
   ],
   "source": [
    "# ==================== PREPROCESS DATA ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Diabetes\n",
    "X_diabetes = df_diabetes.drop(columns=['Diabetes_binary']).values\n",
    "y_diabetes = df_diabetes['Diabetes_binary'].values\n",
    "print(f\"âœ“ Diabetes - Features: {X_diabetes.shape}, Target: {y_diabetes.shape}\")\n",
    "\n",
    "# Adult\n",
    "X_adult_df = df_adult.drop(columns=['income'])\n",
    "y_adult = (df_adult['income'] == '>50K').astype(int).values\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = X_adult_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_adult_df[col] = le.fit_transform(X_adult_df[col].astype(str))\n",
    "\n",
    "# Convert to numpy array\n",
    "X_adult = X_adult_df.values\n",
    "print(f\"âœ“ Adult - Features: {X_adult.shape}, Target: {y_adult.shape}\")\n",
    "\n",
    "DATASETS = {'diabetes': (X_diabetes, y_diabetes), 'adult': (X_adult, y_adult)}\n",
    "MODEL_TYPES = ['LR', 'FNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83694285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODEL ARCHITECTURES ====================\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64], output_size=2, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate model and return accuracy, f1\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    accuracy = accuracy_score(y, predicted.cpu().numpy())\n",
    "    f1 = f1_score(y, predicted.cpu().numpy(), average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "print(\"âœ“ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0849c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIFFERENTIAL PRIVACY WITH CROSS-VALIDATION (CONTINUATION)\n",
      "================================================================================\n",
      "\n",
      "Results will be saved to: /kaggle/working/models_research_dp_continue\n",
      "\n",
      "âœ“ Resuming from: diabetes LR Îµ=5.0, Run 3 onwards\n",
      "  Total remaining: 13 configs + 3 runs from partial config\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: DIABETES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Model: LR\n",
      "\n",
      "    Target Îµ: 0.5\n",
      "      â­ï¸  SKIPPING (before resume point)\n",
      "\n",
      "    Target Îµ: 1.0\n",
      "      â­ï¸  SKIPPING (before resume point)\n",
      "\n",
      "    Target Îµ: 3.0\n",
      "      â­ï¸  SKIPPING (before resume point)\n",
      "\n",
      "    Target Îµ: 5.0 - ðŸ”„ RESUMING FROM RUN 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Run 3, Fold 5: Acc=0.8611, Îµ=0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Run 4, Fold 5: Acc=0.8598, Îµ=0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(DP_EPOCHS):\n\u001b[32m    122\u001b[39m     dp_model.train()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader_dp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\opacus\\data_loader.py:58\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn, sample_empty_shapes, dtypes)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03mWraps `collate_fn` to handle empty batches.\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m \u001b[33;03m    Batch tensor(s)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     61\u001b[39m         torch.zeros(shape, dtype=dtype)\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m shape, dtype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sample_empty_shapes, dtypes)\n\u001b[32m     63\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\almir\\ai-privacy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==================== DIFFERENTIAL PRIVACY TRAINING WITH CHECKPOINT ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIFFERENTIAL PRIVACY WITH CROSS-VALIDATION (CONTINUATION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create models directory\n",
    "try:\n",
    "    models_dir = \"/kaggle/working/models_research_dp_continue\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "except:\n",
    "    models_dir = os.path.join(r\"c:\\Users\\almir\\ai-privacy\\backend\", \"models_research_dp_continue\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nResults will be saved to: {models_dir}\")\n",
    "\n",
    "# Initialize with completed configs\n",
    "dp_results = {\n",
    "    # Diabetes LR - completed\n",
    "    'diabetes_LR_DP_eps5.0': {'dataset': 'diabetes', 'model': 'LR', 'target_epsilon': 5.0},\n",
    "    'diabetes_LR_DP_eps10.0': {'dataset': 'diabetes', 'model': 'LR', 'target_epsilon': 10.0},\n",
    "    # Diabetes FNN - completed\n",
    "    'diabetes_FNN_DP_eps0.5': {'dataset': 'diabetes', 'model': 'FNN', 'target_epsilon': 0.5}\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ“ Skipping already completed:\")\n",
    "print(f\"  - Diabetes LR: Îµ=5.0, Îµ=10.0\")\n",
    "print(f\"  - Diabetes FNN: Îµ=0.5\")\n",
    "print(f\"\\nâœ“ Will run:\")\n",
    "print(f\"  - Diabetes LR: Îµ=0.5, 1.0, 3.0\")\n",
    "print(f\"  - Diabetes FNN: Îµ=1.0, 3.0, 5.0, 10.0\")\n",
    "print(f\"  - Adult LR: all Îµ values\")\n",
    "print(f\"  - Adult FNN: all Îµ values\")\n",
    "print(f\"  Total: 19 configurations Ã— 25 evaluations = 475 evaluations\")\n",
    "\n",
    "checkpoint_path = os.path.join(models_dir, 'dp_checkpoint.json')\n",
    "\n",
    "for dataset_name, (X_data, y_data) in DATASETS.items():\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for model_type in MODEL_TYPES:\n",
    "        print(f\"\\n  Model: {model_type}\")\n",
    "        \n",
    "        for target_epsilon in EPSILON_VALUES:\n",
    "            config_key = f\"{dataset_name}_{model_type}_DP_eps{target_epsilon}\"\n",
    "            \n",
    "            # Skip if already completed\n",
    "            if config_key in dp_results and 'accuracy' in dp_results[config_key]:\n",
    "                print(f\"\\n    Target Îµ: {target_epsilon} - âœ“ SKIPPED (already completed)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n    Target Îµ: {target_epsilon}\")\n",
    "            \n",
    "            all_accuracies = []\n",
    "            all_f1s = []\n",
    "            all_epsilons = []\n",
    "            \n",
    "            for run_idx in range(len(RANDOM_SEEDS)):\n",
    "                seed = RANDOM_SEEDS[run_idx]\n",
    "                skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "                \n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "                    X_train, X_val = X_data[train_idx], X_data[val_idx]\n",
    "                    y_train, y_val = y_data[train_idx], y_data[val_idx]\n",
    "                    \n",
    "                    # Scale features\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_scaled = scaler.fit_transform(X_train)\n",
    "                    X_val_scaled = scaler.transform(X_val)\n",
    "                    \n",
    "                    # Create DataLoader\n",
    "                    train_dataset = TensorDataset(\n",
    "                        torch.FloatTensor(X_train_scaled),\n",
    "                        torch.LongTensor(y_train)\n",
    "                    )\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    input_size = X_train_scaled.shape[1]\n",
    "                    if model_type == 'LR':\n",
    "                        model = LogisticRegressionModel(input_size, output_size=2)\n",
    "                    else:\n",
    "                        model = FeedforwardNN(input_size, hidden_sizes=[128, 64], output_size=2)\n",
    "                    \n",
    "                    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "                    loss_fn = nn.CrossEntropyLoss()\n",
    "                    \n",
    "                    # Attach DP\n",
    "                    privacy_engine = PrivacyEngine()\n",
    "                    dp_model, optimizer, train_loader_dp = privacy_engine.make_private(\n",
    "                        module=model,\n",
    "                        optimizer=optimizer,\n",
    "                        data_loader=train_loader,\n",
    "                        noise_multiplier=DP_NOISE_MULTIPLIER,\n",
    "                        max_grad_norm=DP_MAX_GRAD_NORM,\n",
    "                    )\n",
    "                    \n",
    "                    # Training\n",
    "                    current_epsilon = 0\n",
    "                    for epoch in range(DP_EPOCHS):\n",
    "                        dp_model.train()\n",
    "                        for batch_x, batch_y in train_loader_dp:\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = dp_model(batch_x)\n",
    "                            loss = loss_fn(outputs, batch_y)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        \n",
    "                        current_epsilon = privacy_engine.get_epsilon(DP_DELTA)\n",
    "                        \n",
    "                        # Stop if reached target epsilon\n",
    "                        if current_epsilon >= target_epsilon:\n",
    "                            break\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    accuracy, f1 = evaluate_model(dp_model, X_val_scaled, y_val)\n",
    "                    all_accuracies.append(accuracy)\n",
    "                    all_f1s.append(f1)\n",
    "                    all_epsilons.append(current_epsilon)\n",
    "                    \n",
    "                    if fold_idx == N_SPLITS - 1:\n",
    "                        print(f\"      Run {run_idx + 1}, Fold {fold_idx + 1}: Acc={accuracy:.4f}, Îµ={current_epsilon:.3f}\")\n",
    "            \n",
    "            # Statistics\n",
    "            acc_mean = np.mean(all_accuracies)\n",
    "            acc_std = np.std(all_accuracies, ddof=1)\n",
    "            acc_min = np.min(all_accuracies)\n",
    "            acc_max = np.max(all_accuracies)\n",
    "            f1_mean = np.mean(all_f1s)\n",
    "            f1_std = np.std(all_f1s, ddof=1)\n",
    "            eps_mean = np.mean(all_epsilons)\n",
    "            \n",
    "            dp_results[config_key] = {\n",
    "                'dataset': dataset_name,\n",
    "                'model': model_type,\n",
    "                'target_epsilon': target_epsilon,\n",
    "                'actual_epsilon': eps_mean,\n",
    "                'accuracy': {'mean': acc_mean, 'std': acc_std, 'min': acc_min, 'max': acc_max},\n",
    "                'f1': {'mean': f1_mean, 'std': f1_std},\n",
    "                'all_accuracies': all_accuracies,\n",
    "                'all_f1s': all_f1s\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n    âœ“ DP Îµ={target_epsilon} Results:\")\n",
    "            print(f\"      Accuracy: {acc_mean*100:.2f}% Â± {acc_std*100:.2f}% (range: {acc_min*100:.2f}% - {acc_max*100:.2f}%)\")\n",
    "            print(f\"      F1-Score: {f1_mean*100:.2f}% Â± {f1_std*100:.2f}%\")\n",
    "            print(f\"      Actual Îµ: {eps_mean:.3f}\")\n",
    "            \n",
    "            # Save checkpoint after each config\n",
    "            checkpoint_data = {\n",
    "                'dp_results': {k: v for k, v in dp_results.items() if 'accuracy' in v},\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'last_completed': config_key\n",
    "            }\n",
    "            with open(checkpoint_path, 'w') as f:\n",
    "                json.dump(checkpoint_data, f, indent=2, default=lambda x: float(x) if isinstance(x, np.floating) else x)\n",
    "            print(f\"      ðŸ’¾ Checkpoint saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIFFERENTIAL PRIVACY PHASE COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26471f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STATISTICAL ANALYSIS & COMPARISON ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load baseline results\n",
    "try:\n",
    "    baseline_path = \"/kaggle/input/ai-privacy-baseline-results/research_results.json\"\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "except:\n",
    "    baseline_path = os.path.join(r\"c:\\Users\\almir\\ai-privacy\\backend\", \"models_research\", \"research_results.json\")\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "\n",
    "baseline_results = {}\n",
    "for dataset in ['diabetes', 'adult']:\n",
    "    for model in ['LR', 'FNN']:\n",
    "        key = f\"{dataset}_{model}\"\n",
    "        baseline_results[key] = {\n",
    "            'accuracy': baseline_data['baseline_results'][dataset][model]['accuracy']['mean'],\n",
    "            'all_accuracies': baseline_data['baseline_results'][dataset][model]['all_accuracies']\n",
    "        }\n",
    "\n",
    "print(\"\\nâœ“ Baseline results loaded\")\n",
    "\n",
    "# DP vs Baseline comparisons\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIFFERENTIAL PRIVACY vs BASELINE - Statistical Tests\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dp_comparison = []\n",
    "for config_key, dp_data in dp_results.items():\n",
    "    # Skip placeholder entries\n",
    "    if 'accuracy' not in dp_data or 'all_accuracies' not in dp_data:\n",
    "        continue\n",
    "    \n",
    "    baseline_key = f\"{dp_data['dataset']}_{dp_data['model']}\"\n",
    "    baseline_acc = baseline_results[baseline_key]['accuracy']\n",
    "    baseline_all = baseline_results[baseline_key]['all_accuracies']\n",
    "    \n",
    "    dp_acc = dp_data['accuracy']['mean']\n",
    "    dp_all = dp_data['all_accuracies']\n",
    "    accuracy_loss = baseline_acc - dp_acc\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value = stats.ttest_ind(baseline_all, dp_all)\n",
    "    \n",
    "    dp_comparison.append({\n",
    "        'Dataset': dp_data['dataset'],\n",
    "        'Model': dp_data['model'],\n",
    "        'Epsilon': dp_data['target_epsilon'],\n",
    "        'DP_Accuracy': dp_acc * 100,\n",
    "        'DP_Std': dp_data['accuracy']['std'] * 100,\n",
    "        'Baseline': baseline_acc * 100,\n",
    "        'Accuracy_Loss': accuracy_loss * 100,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "dp_comparison_df = pd.DataFrame(dp_comparison)\n",
    "print(\"\\n\" + dp_comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c46163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save comprehensive JSON\n",
    "results_json = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'random_seeds': RANDOM_SEEDS,\n",
    "        'n_splits': N_SPLITS,\n",
    "        'total_evaluations': len(RANDOM_SEEDS) * N_SPLITS,\n",
    "        'epsilon_values': EPSILON_VALUES,\n",
    "        'dp_epochs': DP_EPOCHS,\n",
    "        'note': 'Continued from diabetes LR Îµ=5.0 Run 3'\n",
    "    },\n",
    "    'differential_privacy': {k: v for k, v in dp_results.items() if 'accuracy' in v},\n",
    "    'baseline_reference': baseline_results\n",
    "}\n",
    "\n",
    "json_path = os.path.join(models_dir, 'dp_continue_results.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results_json, f, indent=2, default=lambda x: float(x) if isinstance(x, np.floating) else x)\n",
    "print(f\"âœ“ Saved: dp_continue_results.json\")\n",
    "\n",
    "# Save comparison CSV\n",
    "dp_csv_path = os.path.join(models_dir, 'dp_vs_baseline.csv')\n",
    "dp_comparison_df.to_csv(dp_csv_path, index=False)\n",
    "print(f\"âœ“ Saved: dp_vs_baseline.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL RESULTS SAVED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATIONS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DP Privacy-Accuracy Tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Differential Privacy: Privacy-Accuracy Tradeoff - 5-Fold CV Ã— 5 Runs (Continued)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, dataset in enumerate(['diabetes', 'adult']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for model in ['LR', 'FNN']:\n",
    "        subset = dp_comparison_df[(dp_comparison_df['Dataset'] == dataset) & (dp_comparison_df['Model'] == model)]\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        \n",
    "        ax.errorbar(subset['Epsilon'], subset['DP_Accuracy'], yerr=subset['DP_Std'], \n",
    "                    marker='o', capsize=5, label=model, linewidth=2, markersize=8)\n",
    "    \n",
    "    # Baseline line\n",
    "    baseline_lr = baseline_results[f\"{dataset}_LR\"]['accuracy'] * 100\n",
    "    baseline_fnn = baseline_results[f\"{dataset}_FNN\"]['accuracy'] * 100\n",
    "    ax.axhline(y=baseline_lr, color='blue', linestyle='--', alpha=0.5, label='LR Baseline')\n",
    "    ax.axhline(y=baseline_fnn, color='orange', linestyle='--', alpha=0.5, label='FNN Baseline')\n",
    "    \n",
    "    ax.set_xlabel('Privacy Budget (Îµ)', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title(f'{dataset.upper()}', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "dp_viz_path = os.path.join(models_dir, 'dp_privacy_accuracy_tradeoff.png')\n",
    "plt.savefig(dp_viz_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ“ Saved: dp_privacy_accuracy_tradeoff.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATIONS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb7d16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook continues DP training from where it stopped:\n",
    "\n",
    "1. **Skipped**: diabetes LR Îµ=[0.5, 1.0, 3.0] (already completed)\n",
    "2. **Resumed**: diabetes LR Îµ=5.0 from Run 3 onwards\n",
    "3. **Remaining**: diabetes LR Îµ=10.0 + diabetes FNN (all Îµ) + adult LR (all Îµ) + adult FNN (all Îµ)\n",
    "4. **Total**: 16 configurations (3 partial + 13 full = ~390 evaluations)\n",
    "5. **Checkpoint**: Saves after each config for crash recovery\n",
    "\n",
    "Estimated runtime: 3-4 hours on Kaggle GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
