{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae261fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== INSTALL DEPENDENCIES ====================\n",
    "# Install opacus for Differential Privacy\n",
    "import sys\n",
    "!{sys.executable} -m pip install --no-cache-dir opacus\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61eca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPORTS ====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import kagglehub\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from opacus import PrivacyEngine\n",
    "import glob\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEEDS = [42, 123, 456, 789, 1011]\n",
    "N_SPLITS = 5\n",
    "DP_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_VALUES = [0.5, 1.0, 3.0, 5.0, 10.0]\n",
    "DP_NOISE_MULTIPLIER = 1.0\n",
    "DP_MAX_GRAD_NORM = 1.0\n",
    "DP_DELTA = 1e-5\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Random seeds: {RANDOM_SEEDS}\")\n",
    "print(f\"  K-fold splits: {N_SPLITS}\")\n",
    "print(f\"  Privacy budgets (ε): {EPSILON_VALUES}\")\n",
    "print(f\"  Total evaluations per config: {len(RANDOM_SEEDS)} × {N_SPLITS} = {len(RANDOM_SEEDS) * N_SPLITS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e29b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LOAD DATASETS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATASETS FROM KAGGLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try Kaggle's native dataset access first (for Kaggle notebooks)\n",
    "try:\n",
    "    # On Kaggle, datasets are mounted at /kaggle/input/\n",
    "    diabetes_paths = glob.glob('/kaggle/input/*/diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "    adult_paths = glob.glob('/kaggle/input/*/adult.csv')\n",
    "    \n",
    "    if diabetes_paths and adult_paths:\n",
    "        diabetes_csv = diabetes_paths[0]\n",
    "        adult_csv = adult_paths[0]\n",
    "        print(\"✓ Using Kaggle native dataset paths\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Datasets not found in /kaggle/input/\")\n",
    "        \n",
    "except (FileNotFoundError, IndexError):\n",
    "    # Fallback to kagglehub for local execution\n",
    "    print(\"✓ Using kagglehub for dataset download\")\n",
    "    \n",
    "    diabetes_path = kagglehub.dataset_download(\"alexteboul/diabetes-health-indicators-dataset\")\n",
    "    diabetes_csv = f\"{diabetes_path}/diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
    "    \n",
    "    adult_path = kagglehub.dataset_download(\"uciml/adult-census-income\")\n",
    "    adult_csv = f\"{adult_path}/adult.csv\"\n",
    "\n",
    "# Load datasets\n",
    "df_diabetes = pd.read_csv(diabetes_csv)\n",
    "print(f\"✓ Diabetes dataset loaded: {df_diabetes.shape}\")\n",
    "\n",
    "df_adult = pd.read_csv(adult_csv)\n",
    "print(f\"✓ Adult dataset loaded: {df_adult.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PREPROCESS DATA ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Diabetes\n",
    "X_diabetes = df_diabetes.drop(columns=['Diabetes_binary']).values\n",
    "y_diabetes = df_diabetes['Diabetes_binary'].values\n",
    "print(f\"✓ Diabetes - Features: {X_diabetes.shape}, Target: {y_diabetes.shape}\")\n",
    "\n",
    "# Adult\n",
    "X_adult_df = df_adult.drop(columns=['income'])\n",
    "y_adult = (df_adult['income'] == '>50K').astype(int).values\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = X_adult_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_adult_df[col] = le.fit_transform(X_adult_df[col].astype(str))\n",
    "\n",
    "# Convert to numpy array\n",
    "X_adult = X_adult_df.values\n",
    "print(f\"✓ Adult - Features: {X_adult.shape}, Target: {y_adult.shape}\")\n",
    "\n",
    "DATASETS = {'diabetes': (X_diabetes, y_diabetes), 'adult': (X_adult, y_adult)}\n",
    "MODEL_TYPES = ['LR', 'FNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MODEL ARCHITECTURES ====================\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64], output_size=2, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate model and return accuracy, f1\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    accuracy = accuracy_score(y, predicted.cpu().numpy())\n",
    "    f1 = f1_score(y, predicted.cpu().numpy(), average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "print(\"✓ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2563a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DIFFERENTIAL PRIVACY TRAINING ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIFFERENTIAL PRIVACY WITH CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create models directory\n",
    "try:\n",
    "    models_dir = \"/kaggle/working/models_research_dp\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "except:\n",
    "    models_dir = os.path.join(r\"c:\\Users\\almir\\ai-privacy\\backend\", \"models_research_dp\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nResults will be saved to: {models_dir}\")\n",
    "print(f\"Total configurations: {len(DATASETS)} datasets × {len(MODEL_TYPES)} models × {len(EPSILON_VALUES)} ε values = {len(DATASETS) * len(MODEL_TYPES) * len(EPSILON_VALUES)}\")\n",
    "\n",
    "dp_results = {}\n",
    "\n",
    "for dataset_name, (X_data, y_data) in DATASETS.items():\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for model_type in MODEL_TYPES:\n",
    "        print(f\"\\n  Model: {model_type}\")\n",
    "        \n",
    "        for target_epsilon in EPSILON_VALUES:\n",
    "            print(f\"\\n    Target ε: {target_epsilon}\")\n",
    "            \n",
    "            all_accuracies = []\n",
    "            all_f1s = []\n",
    "            all_epsilons = []\n",
    "            \n",
    "            for run_idx, seed in enumerate(RANDOM_SEEDS):\n",
    "                skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=seed)\n",
    "                \n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "                    X_train, X_val = X_data[train_idx], X_data[val_idx]\n",
    "                    y_train, y_val = y_data[train_idx], y_data[val_idx]\n",
    "                    \n",
    "                    # Scale features\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train_scaled = scaler.fit_transform(X_train)\n",
    "                    X_val_scaled = scaler.transform(X_val)\n",
    "                    \n",
    "                    # Create DataLoader\n",
    "                    train_dataset = TensorDataset(\n",
    "                        torch.FloatTensor(X_train_scaled),\n",
    "                        torch.LongTensor(y_train)\n",
    "                    )\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "                    \n",
    "                    # Initialize model\n",
    "                    input_size = X_train_scaled.shape[1]\n",
    "                    if model_type == 'LR':\n",
    "                        model = LogisticRegressionModel(input_size, output_size=2)\n",
    "                    else:\n",
    "                        model = FeedforwardNN(input_size, hidden_sizes=[128, 64], output_size=2)\n",
    "                    \n",
    "                    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "                    loss_fn = nn.CrossEntropyLoss()\n",
    "                    \n",
    "                    # Attach DP\n",
    "                    privacy_engine = PrivacyEngine()\n",
    "                    dp_model, optimizer, train_loader_dp = privacy_engine.make_private(\n",
    "                        module=model,\n",
    "                        optimizer=optimizer,\n",
    "                        data_loader=train_loader,\n",
    "                        noise_multiplier=DP_NOISE_MULTIPLIER,\n",
    "                        max_grad_norm=DP_MAX_GRAD_NORM,\n",
    "                    )\n",
    "                    \n",
    "                    # Training\n",
    "                    current_epsilon = 0\n",
    "                    for epoch in range(DP_EPOCHS):\n",
    "                        dp_model.train()\n",
    "                        for batch_x, batch_y in train_loader_dp:\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = dp_model(batch_x)\n",
    "                            loss = loss_fn(outputs, batch_y)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        \n",
    "                        current_epsilon = privacy_engine.get_epsilon(DP_DELTA)\n",
    "                        \n",
    "                        # Stop if reached target epsilon\n",
    "                        if current_epsilon >= target_epsilon:\n",
    "                            break\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    accuracy, f1 = evaluate_model(dp_model, X_val_scaled, y_val)\n",
    "                    all_accuracies.append(accuracy)\n",
    "                    all_f1s.append(f1)\n",
    "                    all_epsilons.append(current_epsilon)\n",
    "                    \n",
    "                    if fold_idx == N_SPLITS - 1:\n",
    "                        print(f\"      Run {run_idx + 1}, Fold {fold_idx + 1}: Acc={accuracy:.4f}, ε={current_epsilon:.3f}\")\n",
    "            \n",
    "            # Statistics\n",
    "            acc_mean = np.mean(all_accuracies)\n",
    "            acc_std = np.std(all_accuracies, ddof=1)\n",
    "            acc_min = np.min(all_accuracies)\n",
    "            acc_max = np.max(all_accuracies)\n",
    "            f1_mean = np.mean(all_f1s)\n",
    "            f1_std = np.std(all_f1s, ddof=1)\n",
    "            eps_mean = np.mean(all_epsilons)\n",
    "            \n",
    "            config_key = f\"{dataset_name}_{model_type}_DP_eps{target_epsilon}\"\n",
    "            dp_results[config_key] = {\n",
    "                'dataset': dataset_name,\n",
    "                'model': model_type,\n",
    "                'target_epsilon': target_epsilon,\n",
    "                'actual_epsilon': eps_mean,\n",
    "                'accuracy': {'mean': acc_mean, 'std': acc_std, 'min': acc_min, 'max': acc_max},\n",
    "                'f1': {'mean': f1_mean, 'std': f1_std},\n",
    "                'all_accuracies': all_accuracies,\n",
    "                'all_f1s': all_f1s\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n    ✓ DP ε={target_epsilon} Results:\")\n",
    "            print(f\"      Accuracy: {acc_mean*100:.2f}% ± {acc_std*100:.2f}% (range: {acc_min*100:.2f}% - {acc_max*100:.2f}%)\")\n",
    "            print(f\"      F1-Score: {f1_mean*100:.2f}% ± {f1_std*100:.2f}%\")\n",
    "            print(f\"      Actual ε: {eps_mean:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIFFERENTIAL PRIVACY PHASE COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31595379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STATISTICAL ANALYSIS & COMPARISON ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load baseline results\n",
    "try:\n",
    "    baseline_path = \"/kaggle/input/ai-privacy-baseline-results/research_results.json\"\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "except:\n",
    "    baseline_path = os.path.join(r\"c:\\Users\\almir\\ai-privacy\\backend\", \"models_research\", \"research_results.json\")\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "\n",
    "baseline_results = {}\n",
    "for dataset in ['diabetes', 'adult']:\n",
    "    for model in ['LR', 'FNN']:\n",
    "        key = f\"{dataset}_{model}\"\n",
    "        baseline_results[key] = {\n",
    "            'accuracy': baseline_data['baseline_results'][dataset][model]['accuracy']['mean'],\n",
    "            'all_accuracies': baseline_data['baseline_results'][dataset][model]['all_accuracies']\n",
    "        }\n",
    "\n",
    "print(\"\\n✓ Baseline results loaded\")\n",
    "\n",
    "# DP vs Baseline comparisons\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIFFERENTIAL PRIVACY vs BASELINE - Statistical Tests\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dp_comparison = []\n",
    "for config_key, dp_data in dp_results.items():\n",
    "    baseline_key = f\"{dp_data['dataset']}_{dp_data['model']}\"\n",
    "    baseline_acc = baseline_results[baseline_key]['accuracy']\n",
    "    baseline_all = baseline_results[baseline_key]['all_accuracies']\n",
    "    \n",
    "    dp_acc = dp_data['accuracy']['mean']\n",
    "    dp_all = dp_data['all_accuracies']\n",
    "    accuracy_loss = baseline_acc - dp_acc\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value = stats.ttest_ind(baseline_all, dp_all)\n",
    "    \n",
    "    dp_comparison.append({\n",
    "        'Dataset': dp_data['dataset'],\n",
    "        'Model': dp_data['model'],\n",
    "        'Epsilon': dp_data['target_epsilon'],\n",
    "        'DP_Accuracy': dp_acc * 100,\n",
    "        'DP_Std': dp_data['accuracy']['std'] * 100,\n",
    "        'Baseline': baseline_acc * 100,\n",
    "        'Accuracy_Loss': accuracy_loss * 100,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "dp_comparison_df = pd.DataFrame(dp_comparison)\n",
    "print(\"\\n\" + dp_comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610865f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save comprehensive JSON\n",
    "results_json = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'random_seeds': RANDOM_SEEDS,\n",
    "        'n_splits': N_SPLITS,\n",
    "        'total_evaluations': len(RANDOM_SEEDS) * N_SPLITS,\n",
    "        'epsilon_values': EPSILON_VALUES,\n",
    "        'dp_epochs': DP_EPOCHS\n",
    "    },\n",
    "    'differential_privacy': dp_results,\n",
    "    'baseline_reference': baseline_results\n",
    "}\n",
    "\n",
    "json_path = os.path.join(models_dir, 'dp_research_results.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results_json, f, indent=2, default=lambda x: float(x) if isinstance(x, np.floating) else x)\n",
    "print(f\"✓ Saved: dp_research_results.json\")\n",
    "\n",
    "# Save comparison CSV\n",
    "dp_csv_path = os.path.join(models_dir, 'dp_vs_baseline.csv')\n",
    "dp_comparison_df.to_csv(dp_csv_path, index=False)\n",
    "print(f\"✓ Saved: dp_vs_baseline.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL RESULTS SAVED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATIONS ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DP Privacy-Accuracy Tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Differential Privacy: Privacy-Accuracy Tradeoff - 5-Fold CV × 5 Runs', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, dataset in enumerate(['diabetes', 'adult']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for model in ['LR', 'FNN']:\n",
    "        subset = dp_comparison_df[(dp_comparison_df['Dataset'] == dataset) & (dp_comparison_df['Model'] == model)]\n",
    "        \n",
    "        ax.errorbar(subset['Epsilon'], subset['DP_Accuracy'], yerr=subset['DP_Std'], \n",
    "                    marker='o', capsize=5, label=model, linewidth=2, markersize=8)\n",
    "    \n",
    "    # Baseline line\n",
    "    baseline_lr = baseline_results[f\"{dataset}_LR\"]['accuracy'] * 100\n",
    "    baseline_fnn = baseline_results[f\"{dataset}_FNN\"]['accuracy'] * 100\n",
    "    ax.axhline(y=baseline_lr, color='blue', linestyle='--', alpha=0.5, label='LR Baseline')\n",
    "    ax.axhline(y=baseline_fnn, color='orange', linestyle='--', alpha=0.5, label='FNN Baseline')\n",
    "    \n",
    "    ax.set_xlabel('Privacy Budget (ε)', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title(f'{dataset.upper()}', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "dp_viz_path = os.path.join(models_dir, 'dp_privacy_accuracy_tradeoff.png')\n",
    "plt.savefig(dp_viz_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: dp_privacy_accuracy_tradeoff.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATIONS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa65f7a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has completed:\n",
    "\n",
    "1. **Differential Privacy**: 5-fold CV × 5 runs for 5 epsilon values × 2 datasets × 2 models = 20 configurations (500 total evaluations)\n",
    "2. **Statistical Analysis**: T-tests comparing DP against baseline with p-values\n",
    "3. **Results Export**: JSON and CSV files with comprehensive statistics\n",
    "4. **Visualizations**: Privacy-accuracy tradeoff charts showing mean ± std\n",
    "\n",
    "All results are publication-ready with proper statistical rigor."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
